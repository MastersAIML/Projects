{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll implement an L-layered deep neural network and train it on the MNIST dataset. The MNIST dataset contains scanned images of handwritten digits, along with their correct classification labels (between 0-9). MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The number of examples in the training dataset is:50000\n",
      "The number of points in a single input is:784\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature dataset is:\" + str(training_data[0]))\n",
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
    "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as discussed earlier in the lectures, the target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # input is the target dataset of shape (m,) where m is the number of data points\n",
    "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
    "    # Look at the next block of code for a better understanding of one hot encoding\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(data.shape)\n",
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function data_wrapper() will convert the dataset into the desired shape and also convert the ground truth labels to one_hot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (784, 50000)\n",
      "train_set_y shape: (10, 50000)\n",
      "test_set_x shape: (784, 10000)\n",
      "test_set_y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The one hot encoding dataset is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49990</th>\n",
       "      <th>49991</th>\n",
       "      <th>49992</th>\n",
       "      <th>49993</th>\n",
       "      <th>49994</th>\n",
       "      <th>49995</th>\n",
       "      <th>49996</th>\n",
       "      <th>49997</th>\n",
       "      <th>49998</th>\n",
       "      <th>49999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 50000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      \\\n",
       "0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0   \n",
       "2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   ...    49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n",
       "0  ...      0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "1  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2  ...      0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4  ...      0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n",
       "5  ...      0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "6  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "7  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "8  ...      1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n",
       "9  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 50000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The one hot encoding dataset is:\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1118ddba8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111789208>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 1000\n",
    "k = train_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, the matrix **Z**, as the input. Upon application of the **`sigmoid`** function, the output matrix **H** is calculated. Also, **Z** is stored as the variable **sigmoid_memory** since it will be later used in backpropagation.You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    sigmoid_memory = Z\n",
    "    \n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "sigmoid(Z) = (array([[0.5       , 0.73105858],\n",
      "       [0.88079708, 0.95257413],\n",
      "       [0.98201379, 0.99330715],\n",
      "       [0.99752738, 0.99908895]]), array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.arange(8).reshape(4,2)\n",
    "print (Z)\n",
    "print (\"sigmoid(Z) = \" + str(sigmoid(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`relu`** function, matrix **H** which is the output matrix is calculated. Also, **Z** is stored as **relu_memory** which will be later used in backpropagation. You use _[np.maximum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)_ here in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = np.maximum(0,Z)\n",
    "    \n",
    "    assert(H.shape == Z.shape)\n",
    "    \n",
    "    relu_memory = Z \n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu(Z) = (array([[ 1,  3],\n",
      "       [ 0,  0],\n",
      "       [ 0,  7],\n",
      "       [ 9, 18]]), array([[ 1,  3],\n",
      "       [-1, -4],\n",
      "       [-5,  7],\n",
      "       [ 9, 18]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4,2)\n",
    "print (\"relu(Z) = \" + str(relu(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "This is the activation of the last layer. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`softmax`** function, the output matrix **H** is calculated. Also, **Z** is stored as **softmax_memory** which will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ and _[np.sum()](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "   \n",
    "    Z_exp = np.exp(Z)\n",
    "\n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    H = Z_exp/Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "    \n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[11,19,10], [12, 21, 23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.68941421e-01 1.19202922e-01 2.26032430e-06]\n",
      " [7.31058579e-01 8.80797078e-01 9.99997740e-01]]\n",
      "[[11 19 10]\n",
      " [12 21 23]]\n"
     ]
    }
   ],
   "source": [
    "#Z = np.array(np.arange(30)).reshape(10,3)\n",
    "H, softmax_memory = softmax(Z)\n",
    "print(H)\n",
    "print(softmax_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters\n",
    "Let's now create a function **`initialize_parameters`** which initializes the weights and biases of the various layers. One way to initialise is to set all the parameters to 0. This is not a considered a good strategy as all the neurons will behave the same way and it'll defeat the purpose of deep networks. Hence, we initialize the weights randomly to very small values but not zeros. The biases are initialized to 0. Note that the **`initialize_parameters`** function initializes the parameters for all the layers in one `for` loop. \n",
    "\n",
    "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
    "\n",
    "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output.\n",
    "\n",
    "Also, notice that the parameters are returned in a dictionary. This will help you in implementing the feedforward through the layer and the backprop throught the layer at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n",
      "  -0.09464469]\n",
      " [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n",
      "   0.01179136]\n",
      " [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n",
      "   0.00545832]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n",
      " [-0.28074571 -0.13967752  0.02641189]\n",
      " [ 0.10925169  0.06646016  0.08565535]\n",
      " [-0.11058228  0.03715795  0.13440124]\n",
      " [-0.16421272 -0.1153127   0.02013163]\n",
      " [ 0.13985659  0.07228733 -0.10717236]\n",
      " [-0.05673344 -0.03663499 -0.15460347]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "dimensions  = [784, 3,7,10]\n",
    "parameters = initialize_parameters(dimensions)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "# print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_forward\n",
    "\n",
    "The function **`layer_forward`** implements the forward propagation for a certain layer 'l'. It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer **H**. It takes **H_prev, W, b and the activation function** as inputs and stores the **linear_memory, activation_memory** in the variable **memory** which will be used later in backpropagation. \n",
    "\n",
    "<br> You have to first calculate the **Z**(using the forward propagation equation), **linear_memory**(H_prev, W, b) and then calculate **H, activation_memory**(Z) by applying activation functions - **`sigmoid`**, **`relu`** and **`softmax`** on **Z**.\n",
    "\n",
    "<br> Note that $$H^{L-1}$$ is referred here as H_prev. You might want to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_ to carry out the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.dot(W,H_prev) + b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.dot(W,H_prev) + b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z = np.dot(W,H_prev) + b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],\n",
       "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    array([[1.        , 1.        , 1.        , 1.        , 1.        ],<br>\n",
    "      [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],<br>\n",
    "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_forward\n",
    "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch). Use the **`layer_forward`** you have created above here to perform the feedforward for layers 1 to 'L-1' in the for loop with the activation **`relu`**. The last layer having a different activation **`softmax`** is calculated outside the loop. Notice that the **memory** is appended to **memories** for all the layers. These will be used in the backward order during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H\n",
    "        H, memory = layer_forward(H_prev, parameters['W' + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        \n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = layer_forward(H, parameters['W' + str(L)], parameters[\"b\" + str(L)], \"softmax\")\n",
    "    \n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]\n",
      " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]\n",
      " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]\n",
      " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]\n",
      " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]\n",
      " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]\n",
      " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]\n",
      " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]\n",
      " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]\n",
      " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "(784, 10)<br>\n",
    "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]<br>\n",
    " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]<br>\n",
    " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]<br>\n",
    " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]<br>\n",
    " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]<br>\n",
    " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]<br>\n",
    " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]<br>\n",
    " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]<br>\n",
    " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]<br>\n",
    " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "### compute_loss\n",
    "The next step is to compute the loss function after every forward pass to keep checking whether it is decreasing with training.<br> **`compute_loss`** here calculates the cross-entropy loss. You may want to use _[np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.multiply()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html)_ here. Do not forget that it is the average loss across all the data points in the batch. It takes the output of the last layer **HL** and the ground truth label **Y** as input and returns the **loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def compute_loss(HL, Y):\n",
    "\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    print (m)\n",
    "\n",
    "    loss = -1./m*np.sum(np.multiply(Y,np.log(HL)))#write your code here, use (1./m) and not (1/m)\n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.8964600261334037\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "# HL is (10, 5), Y is (10, 5)\n",
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_set_y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    \n",
    "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]<br>\n",
    " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]<br>\n",
    " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]<br>\n",
    " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]<br>\n",
    " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]<br>\n",
    " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]<br>\n",
    " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]<br>\n",
    " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]<br>\n",
    " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]<br>\n",
    " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]<br>\n",
    "[[0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 1.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [1. 0. 1. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 1. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 1. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]]<br>\n",
    "0.8964600261334037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Let's now get to the next step - backpropagation. Let's start with sigmoid_backward.\n",
    "\n",
    "### sigmoid-backward\n",
    "You might remember that we had created **`sigmoid`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **sigmoid_memory** as input. **sigmoid_memory** is the **Z** which we had calculated during forward propagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu-backward\n",
    "You might remember that we had created **`relu`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **relu_memory** as input. **relu_memory** is the **Z** which we calculated uring forward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_backward\n",
    "\n",
    "**`layer_backward`** is a complimentary function of **`layer_forward`**. Like **`layer_forward`** calculates **H** using **W**, **H_prev** and **b**, **`layer_backward`** uses **dH** to calculate **dW**, **dH_prev** and **db**. You have already studied the formulae in backpropogation. To calculate **dZ**, use the **`sigmoid_backward`** and **`relu_backward`** function. You might need to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)_ for the rest. Remember to choose the axis correctly in db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_backward(dH, memory, activation = 'relu'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = 1./m*np.dot(dZ, H_prev.T)\n",
    "        db = 1./m*np.dot(dZ, np.ones([m,1]))\n",
    "        dH_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = 1./m*np.dot(dZ, H_prev.T)\n",
    "        db = 1./m*np.dot(dZ, np.ones([m,1]))\n",
    "        dH_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
      " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
      "dW is \n",
      " [[1.67565336 1.56891359]\n",
      " [1.39137819 1.4143854 ]\n",
      " [1.3597389  1.43013369]]\n",
      "db is \n",
      " [[0.37345476]\n",
      " [0.34414727]\n",
      " [0.29074635]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "dH_prev is <br>\n",
    " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]<br>\n",
    " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]<br>\n",
    "dW is <br>\n",
    " [[1.67565336 1.56891359]<br>\n",
    " [1.39137819 1.4143854 ]<br>\n",
    " [1.3597389  1.43013369]]<br>\n",
    "db is <br>\n",
    " [[0.37345476]<br>\n",
    " [0.34414727]<br>\n",
    " [0.29074635]]<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_backward\n",
    "\n",
    "**`L_layer_backward`** performs backpropagation for the whole network. Recall that the backpropagation for the last layer, i.e. the softmax layer, is different from the rest, hence it is outside the reversed `for` loop. You need to use the function **`layer_backward`** here in the loop with the activation function as **`relu`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    # Use the expressions you have used in 'layer_backward'\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(W.T, dZ)\n",
    "    gradients[\"dW\" + str(L)] = 1./m*np.dot(dZ, H_prev.T) #write your code here, use (1./m) and not (1/m)\n",
    "    gradients[\"db\" + str(L)] = 1./m*np.dot(dZ, np.ones([m,1])) #write your code here, use (1./m) and not (1/m)\n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l+1)], current_memory, 'relu')\n",
    "        \n",
    "        # dZl1 = relu_backward(gradients[\"dH\" + str(l + 1)], current_memory)\n",
    "        # dwl1 = np.dot(dZl1,current_memory[0][0].T)\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp \n",
    "        gradients[\"dW\" + str(l + 1)] = 1./m*dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = 1./m*db_temp\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW3 is \n",
      " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863\n",
      "   0.        ]\n",
      " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533\n",
      "   0.        ]\n",
      " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996\n",
      "   0.        ]\n",
      " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381\n",
      "   0.        ]\n",
      " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682\n",
      "   0.        ]\n",
      " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441\n",
      "   0.        ]\n",
      " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034\n",
      "   0.        ]\n",
      " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535\n",
      "   0.        ]\n",
      " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264\n",
      "   0.        ]\n",
      " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965\n",
      "   0.        ]]\n",
      "db3 is \n",
      " [[ 0.10031756]\n",
      " [ 0.00460183]\n",
      " [-0.00142942]\n",
      " [-0.0997827 ]\n",
      " [ 0.09872663]\n",
      " [ 0.00536378]\n",
      " [-0.10124784]\n",
      " [-0.00191121]\n",
      " [-0.00359044]\n",
      " [-0.00104818]]\n",
      "dW2 is \n",
      " [[ 4.94428956e-06  1.13215514e-03  5.44180380e-03]\n",
      " [-4.81267081e-06 -2.96999448e-06 -1.81899582e-03]\n",
      " [ 5.63424333e-06  4.77190073e-04  4.04810232e-03]\n",
      " [ 1.49767478e-05 -1.89780927e-04 -7.91231369e-04]\n",
      " [ 1.97866094e-05  1.22107085e-05  2.64140566e-03]\n",
      " [ 0.00000000e+00 -3.75805770e-05  1.63906102e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "db2 is \n",
      " [[ 1.39790007e-03]\n",
      " [-1.32938320e-03]\n",
      " [ 1.27570684e-03]\n",
      " [-1.05295652e-03]\n",
      " [ 3.17922449e-03]\n",
      " [-3.98765210e-05]\n",
      " [ 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "y_sample = train_set_y[:, 10:20]\n",
    "\n",
    "HL, memories = L_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = L_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW3'])\n",
    "print('db3 is \\n', gradients['db3'])\n",
    "print('dW2 is \\n', gradients['dW2'])\n",
    "print('db2 is \\n', gradients['db2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "\n",
    "dW3 is <br>\n",
    " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863  0.        ]<br>\n",
    " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533   0.        ]<br>\n",
    " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996   0.        ]<br>\n",
    " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381   0.        ]<br>\n",
    " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682   0.        ]<br>\n",
    " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441   0.        ]<br>\n",
    " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034   0.        ]<br>\n",
    " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535   0.        ]<br>\n",
    " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264   0.        ]<br>\n",
    " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965    0.        ]]<br>\n",
    "db3 is <br>\n",
    " [[ 0.10031756]<br>\n",
    " [ 0.00460183]<br>\n",
    " [-0.00142942]<br>\n",
    " [-0.0997827 ]<br>\n",
    " [ 0.09872663]<br>\n",
    " [ 0.00536378]<br>\n",
    " [-0.10124784]<br>\n",
    " [-0.00191121]<br>\n",
    " [-0.00359044]<br>\n",
    " [-0.00104818]]<br>\n",
    "dW2 is <br>\n",
    " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]<br>\n",
    " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]<br>\n",
    " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]<br>\n",
    " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]<br>\n",
    " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]<br>\n",
    " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]<br>\n",
    " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]<br>\n",
    "db2 is <br>\n",
    " [[ 0.013979  ]<br>\n",
    " [-0.01329383]<br>\n",
    " [ 0.01275707]<br>\n",
    " [-0.01052957]<br>\n",
    " [ 0.03179224]<br>\n",
    " [-0.00039877]<br>\n",
    " [ 0.        ]]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Updates\n",
    "\n",
    "Now that we have calculated the gradients. let's do the last step which is updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*gradients[\"db\" + str(l+1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784, 200, 10] #  three-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### L_layer_model\n",
    "\n",
    "This is a composite function which takes the training data as input **X**, ground truth label **Y**, the **dimensions** as stated above, **learning_rate**, the number of iterations **num_iterations** and if you want to print the loss, **print_loss**. You need to use the final functions we have written for feedforward, computing the loss, backpropagation and updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL, Y, memories)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it'll take a lot of time to train the model on 50,000 data points, we take a subset of 5,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 5000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_new = train_set_x[:,0:5000]\n",
    "train_set_y_new = train_set_y[:,0:5000]\n",
    "train_set_x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the function L_layer_model on the dataset we have created.This will take 10-20 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "Loss after iteration 0: 2.901777\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 100: 2.090649\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 200: 1.747517\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 300: 1.509609\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 400: 1.340862\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 500: 1.216586\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 600: 1.121612\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 700: 1.046687\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 800: 0.985999\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 900: 0.935759\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1000: 0.893409\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1100: 0.857161\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1200: 0.825731\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1300: 0.798175\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1400: 0.773782\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1500: 0.752006\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1600: 0.732423\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1700: 0.714696\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1800: 0.698555\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 1900: 0.683781\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2000: 0.670196\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2100: 0.657650\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2200: 0.646019\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2300: 0.635198\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2400: 0.625098\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2500: 0.615643\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2600: 0.606768\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2700: 0.598416\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2800: 0.590539\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "Loss after iteration 2900: 0.583092\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8HXW9//HXJ/u+NEm3dC9d2CmUnasVEUFAyk9EFBe8+kO84IpecLng8uNerl5cEBFRELgiiqAICCogW9nTvaWU7vuSplmbPfn8/phJehqSNC09mZyc9/PxmEfmzHzPnM/0wHmfmfnO95i7IyIiApASdQEiIjJ0KBRERKSbQkFERLopFEREpJtCQUREuikURESkm0JBhgUze8LMPhV1HSKJTqEg74iZrTezs6Kuw93Pdfd7oq4DwMyeNbPPDsLrZJrZXWZWZ2bbzeyr+2n/lbBdXfi8zJh1k8zsGTNrNLM3Y99TM7vdzBpiphYzq49Z/6yZNcesXxmfPZbBoFCQIc/M0qKuoctQqgX4DjANmAi8B/h3Mzunt4Zm9n7gOuC9YfspwHdjmtwPLARKgG8BD5pZGYC7X+nueV1T2PaPPV7i6pg2Mw7VDsrgUyhI3JjZ+Wa2yMxqzOwlMzsmZt11ZrbGzOrN7A0zuyhm3eVm9qKZ/djMqoDvhMvmmdn/mFm1ma0zs3NjntP97XwAbSeb2fPhaz9lZj83s9/2sQ9zzGyzmV1rZtuB35hZsZk9ZmaV4fYfM7NxYfsbgX8Bbg2/Nd8aLp9pZk+a2W4zW2lmlxyCf+JPAd9392p3XwH8Cri8n7Z3uvtyd68Gvt/V1symA8cDN7h7k7s/BCwFPtTLv0duuHxIHJXJoadQkLgws1nAXcDnCL59/hJ4JOaUxRqCD89Cgm+svzWzMTGbOBlYC4wCboxZthIoBX4A3Glm1kcJ/bX9HfBaWNd3gE/sZ3dGAyMIvmFfQfD/zW/CxxOAJuBWAHf/FvACe785Xx1+kD4Zvu5I4FLgNjM7orcXM7PbwiDtbVoStikGxgCLY566GDiyj304spe2o8ysJFy31t3re6zvbVsfAiqB53ss/y8z2xWG+Zw+apAEoFCQeLkC+KW7v+ruHeH5/hbgFAB3/6O7b3X3Tnf/A7AKOCnm+Vvd/Wfu3u7uTeGyDe7+K3fvIPimOoYgNHrTa1szmwCcCFzv7q3uPg94ZD/70knwLbol/CZd5e4PuXtj+EF6I/Dufp5/PrDe3X8T7s9C4CHgw701dvd/c/eiPqauo6288G9tzFNrgfw+asjrpS1h+57r+tvWp4B7fd9B064lOB1VDtwBPGpmU/uoQ4Y4hYLEy0TgmthvucB4YCyAmX0y5tRSDXAUwbf6Lpt62eb2rhl3bwxn83pp11/bscDumGV9vVasSndv7npgZjlm9ksz22BmdQTfmovMLLWP508ETu7xb3EZwRHIwWoI/xbELCsA6ntp29W+Z1vC9j3X9bqtMFDnAPfGLg+Dvz4MzXuAF4EPDGw3ZKhRKEi8bAJu7PEtN8fd7zeziQTnv68GSty9CFgGxJ4KitfwvduAEWaWE7Ns/H6e07OWa4AZwMnuXgC8K1xufbTfBDzX498iz90/39uL9dLbJ3ZaDhBeF9gGHBvz1GOB5X3sw/Je2u5w96pw3RQzy++xvue2PgG86O5r+3iNLs6+76UkEIWCHArpZpYVM6URfOhfaWYnWyDXzM4LP3hyCT44KgHM7NMERwpx5+4bgAqCi9cZZnYqcMEBbiaf4DpCjZmNAG7osX4HwemULo8B083sE2aWHk4nmtnhfdS4T2+fHlPsef57gW+HF75nAv8XuLuPmu8FPmNmR5hZEfDtrrbu/hawCLghfP8uAo4hOMUV65M9t29mRWb2/q733cwuIwjJv/VRhwxxCgU5FB4n+JDsmr7j7hUEH1K3AtXAasLeLu7+BnAz8DLBB+jRBKccBstlwKlAFfD/gD8QXO8YqJ8A2cAu4BXe/gH4U+DisGfSLeF1h7MJLjBvJTi19d9AJu/MDQQX7DcAzwE/dPe/QXCqJzyymAAQLv8B8AywMXxObJhdCswmeK9uAi5298qulWF4juPtXVHTCf4NKwn+Pb4AzA2DRhKQ6Ud2JNmZ2R+AN9295zd+kaSjIwVJOuGpm6lmlmLBzV4XAg9HXZfIUDCU7s4UGSyjgT8R3KewGfh82E1UJOnp9JGIiHSL2+mjsDfCa2a22MyWm9l3e2mTaWZ/MLPVZvaqmU2KVz0iIrJ/8Tx91AKc6e4NZpYOzDOzJ9z9lZg2nwGq3f0wM7uUoEfGR/rbaGlpqU+aNCluRYuIDEfz58/f5e5l+2sXt1AIb4PvuusyPZx6nqu6kGDsGYAHCQYRM+/nnNakSZOoqKg4xNWKiAxvZrZhIO3i2vvIzFLNbBGwE3jS3V/t0aSccIgBd28nGG+lpJftXGFmFWZWUVlZ2XO1iIgcInENhXAgtOMIbno5ycwO6q5Vd7/D3We7++yysv0e/YiIyEEalPsU3L2G4E7Knj8AsoVw3JlwaIRCgrtMRUQkAvHsfVQWjrGCmWUD7wPe7NHsEYKheAEuBv7Z3/UEERGJr3j2PhoD3BMOJ5wCPODuj5nZ94AKd38EuBP4XzNbDewmGH9FREQiEs/eR0uAWb0svz5mvpk+fmhEREQGn8Y+EhGRbkkTCm9ur+OmJ96krrkt6lJERIaspAmFTbubuP25NazZ2bD/xiIiSSppQmFyaS4Aayv3RFyJiMjQlTShMGFEDqkpxtpdOlIQEelL0oRCRloKE0bk6EhBRKQfSRMKAFNKc1m3S6EgItKX5AqFsiAUOjt107SISG+SLBTyaGnvZEtNU9SliIgMSckVCl09kHQKSUSkV0kVCpPLurqlqgeSiEhvkioUyvIyyc9MUw8kEZE+JFUomBlTynJ1r4KISB+SKhQguNi8TkcKIiK9Sr5QKM1la20zja3tUZciIjLkJF8olOUB6CY2EZFeJGEoaGA8EZG+JF0oTCpRKIiI9CXpQiE7I5Xyomz1QBIR6UXShQIEp5B0pCAi8nbJGQrhaKnuGhhPRCRWcoZCWR4NLe1U1rdEXYqIyJCSpKEQXGxeo1NIIiL7SNJQCO5V0MVmEZF9JWUojCnIIis9RRebRUR6SMpQSEkxJpXkaghtEZEekjIUAKaW5enHdkREekjaUJhSlsum3Y20tndGXYqIyJCR1KHQ6bBxt44WRES6JG8olAY9kNQtVURkr6QNhckaLVVE5G2SNhQKstIpzctUDyQRkRhJGwoQDoynHkgiIt3iFgpmNt7MnjGzN8xsuZl9qZc2c8ys1swWhdP18aqnN1PLdK+CiEistDhuux24xt0XmFk+MN/MnnT3N3q0e8Hdz49jHX2aUppHdWMb1XtaKc7NiKIEEZEhJW5HCu6+zd0XhPP1wAqgPF6vdzC6f5pTp5BERIBBuqZgZpOAWcCrvaw+1cwWm9kTZnZkH8+/wswqzKyisrLykNXVPTCeTiGJiACDEApmlgc8BHzZ3et6rF4ATHT3Y4GfAQ/3tg13v8PdZ7v77LKyskNW2/jibNJSTEcKIiKhuIaCmaUTBMJ97v6nnuvdvc7dG8L5x4F0MyuNZ02x0lJTmFCSoyMFEZFQPHsfGXAnsMLdf9RHm9FhO8zspLCeqnjV1JsppXm6gU1EJBTP3kenA58AlprZonDZN4EJAO5+O3Ax8HkzaweagEt9kH84eWpZLs+/VUlHp5OaYoP50iIiQ07cQsHd5wH9fsq6+63ArfGqYSCmlOXS2tHJluomJpTkRFmKiEjkkvqOZtjbA2mNfppTREShMKVUA+OJiHRJ+lAYkZtBYXa6eiCJiKBQwMyYXJqrIwURERQKQNdoqTpSEBFRKABTy/LYUddCQ0t71KWIiERKocDei83rNdyFiCQ5hQIx3VJ1sVlEkpxCAZhYkoOZuqWKiCgUgKz0VMYVZ2u0VBFJegqF0OTSPN2rICJJT6EQmlKay7pdexjk8fhERIYUhUJoalkuja0dbK9rjroUEZHIKBRCXT2Q1ulis4gkMYVCaEpZcK/CGl1sFpEkplAIjS7IIicjVRebRSSpKRRCGhhPREShsI/JpRoYT0SSm0IhxpSyPDZXN9Hc1hF1KSIikVAoxJhalos7bKhqjLoUEZFIKBRiTCkNu6XqFJKIJCmFQozJXd1SdbFZRJKUQiFGXmYaowoy1QNJRJKWQqGHKaV56oEkIklLodDD5LLgXgUNjCciyUih0MOU0lxqm9rYvac16lJERAadQqGHqeHAePrBHRFJRgqFHroGxtNoqSKSjBQKPYwrziEjNYU1utgsIklIodBDaooxdWQeCzfWRF2KiMigUyj04tyjRvPaut1sqWmKuhQRkUGlUOjF3OPKAXhk0daIKxERGVwKhV5MKMnh+AlFPLxwS9SliIgMKoVCHy6aVc7KHfWs2FYXdSkiIoMmbqFgZuPN7Bkze8PMlpvZl3ppY2Z2i5mtNrMlZnZ8vOo5UOcdM5a0FNPRgogklXgeKbQD17j7EcApwFVmdkSPNucC08LpCuAXcazngIzIzWDOjDL+smgrHZ0a8kJEkkPcQsHdt7n7gnC+HlgBlPdodiFwrwdeAYrMbEy8ajpQc2eVs72umVfXVkVdiojIoBiUawpmNgmYBbzaY1U5sCnm8WbeHhyY2RVmVmFmFZWVlfEq823OOnwUeZlp/FmnkEQkScQ9FMwsD3gI+LK7H9RVW3e/w91nu/vssrKyQ1tgP7LSUzn3qNE8sWy7frdZRJJCXEPBzNIJAuE+d/9TL022AONjHo8Llw0Zc2eV09DSzlMrdkRdiohI3MWz95EBdwIr3P1HfTR7BPhk2AvpFKDW3bfFq6aDccqUEkYVZPLwQt3IJiLDX1oct3068AlgqZktCpd9E5gA4O63A48DHwBWA43Ap+NYz0FJTTEuPK6cu+atY/eeVkbkZkRdkohI3MQtFNx9HmD7aePAVfGq4VCZe1w5dzy/lr8u3cYnTpkYdTkiInGjO5oH4PAx+cwYla8b2URk2FMoDICZMXdWOfM3VLOxqjHqckRE4kahMEAXHjcWgIcX6WhBRIYvhcIAjS3K5uTJI3h40RaCSyEiIsOPQuEAXDSrnLWVe1i6pTbqUkRE4kKhcADOPXoMGakpGvZCRIYthcIBKMxO572Hj+TRxVtp7+iMuhwRkUNOoXCA5s4qZ1dDK/NW74q6FBGRQ06hcIDmzCijMDtd9yyIyLCkUDhAmWmpnHfMGP6+fAd7WtqjLkdE5JAaUCiY2ZfMrCAcuO5OM1tgZmfHu7ih6qJZ5TS1dfDkGxo5VUSGl4EeKfxr+FsIZwPFBAPd3RS3qoa4EyYUU16UrV5IIjLsDDQUuga2+wDwv+6+nP0MdjecpaQYc2eN5YVVlVTWt0RdjojIITPQUJhvZv8gCIW/m1k+kNR9MuceV06nw6OL9TsLIjJ8DDQUPgNcB5zo7o1AOkPwtw8G07RR+RxVXqCxkERkWBloKJwKrHT3GjP7OPBtIOnHeph7XDlLNteyfGvS/1OIyDAx0FD4BdBoZscC1wBrgHvjVlWC+PDs8RRkpfHjJ1dFXYqIyCEx0FBoD38l7ULgVnf/OZAfv7ISQ2F2Ole8awpPrdjBwo3VUZcjIvKODTQU6s3sGwRdUf9qZikE1xWS3qdPn8yI3Ax+9ORbUZciIvKODTQUPgK0ENyvsB0YB/wwblUlkNzMNP5tzlReWLWLV9ZWRV2OiMg7MqBQCIPgPqDQzM4Hmt096a8pdPn4KRMZVZDJzf9YqR/gEZGENtBhLi4BXgM+DFwCvGpmF8ezsESSlZ7K1WdO4/X11Tz3VmXU5YiIHLSBnj76FsE9Cp9y908CJwH/Eb+yEs9HZo9nXHE2N//jLR0tiEjCGmgopLj7zpjHVQfw3KSQkZbCl947jaVbavn7cg2UJyKJaaAf7H8zs7+b2eVmdjnwV+Dx+JWVmC6aVc6Uslx+9ORKOjp1tCAiiWegF5q/DtwBHBNOd7j7tfEsLBGlpabwlbOm89aOBh5bojGRRCTxDPgUkLs/5O5fDac/x7OoRHbe0WOYOTqfHz/5ln7HWUQSTr+hYGb1ZlbXy1RvZnWDVWQiSUkxrjl7BuurGnloweaoyxEROSD9hoK757t7QS9TvrsXDFaRieasw0dy7Pgibnl6NS3tHVGXIyIyYOpBFAdmxtfOns6WmiZ+/9qmqMsRERkwhUKcnHFYKSdNHsGtz6ymqVVHCyKSGBQKcRIcLcygsr6Fe19eH3U5IiIDolCIo5Mmj+Bd08u4/bk11De3RV2OiMh+KRTi7GtnT6e6sY275q2PuhQRkf2KWyiY2V1mttPMlvWxfo6Z1ZrZonC6Pl61ROmYcUWcfcQofv3CWmoaW6MuR0SkX/E8UrgbOGc/bV5w9+PC6XtxrCVSXz17Og2t7dz6z9VRlyIi0q+4hYK7Pw/sjtf2E8nM0QVceuJ47nxxHS+t3hV1OSIifYr6msKpZrbYzJ4wsyP7amRmV5hZhZlVVFYm5u8V/Mf5RzClNJcv/2ERVQ0tUZcjItKrKENhATDR3Y8FfgY83FdDd7/D3We7++yysrJBK/BQyslI42cfPZ6axja+9sfF+s0FERmSIgsFd69z94Zw/nEg3cxKo6pnMBwxtoBvfmAmz6ys5K4X10ddjojI20QWCmY22swsnD8prKUqqnoGy6dOm8RZh4/ipidWsGxLbdTliIjsI55dUu8HXgZmmNlmM/uMmV1pZleGTS4GlpnZYuAW4FJPgnMqZsYPLz6GktxMvnD/Qhpa2qMuSUSkmyXa5/Ds2bO9oqIi6jLesVfWVvGxX73CRbPGcfMlx0ZdjogMc2Y2391n769d1L2PktYpU0q4+sxpPLRgMw8v3BJ1OSIigEIhUl888zBOnFTMt/68lPW79kRdjoiIQiFKaakp/OTSWaSlpvDF3y+ktV0/3yki0VIoRKy8KJv//tAxLNlcy//8Y2XU5YhIklMoDAHnHDWaj58ygTueX8uzK3dGXY6IJDGFwhDx7fOOYMaofL72x8XsrG+OuhwRSVIKhSEiKz2Vn31sFg0t7VzzwGI6OhOrq7CIDA8KhSFk+qh8vnPBkbywahdf/6OCQUQGX1rUBci+Lj1pApX1Ldz85FukpBg/+NAxpKRY1GWJSJJQKAxBX3jvNDrc+clTq0gxuOn/KBhEZHAoFIaoL581nU6HW55eRYoZ/3nR0QoGEYk7hcIQ9pWzpuHu/OyfqzEzbpx7lIJBROJKoTCEmRlffd90Ojqd255dQ2oKfP/CowhHHBcROeQUCkOcmfH198+g0+H259aQYsZ3P3ikgkFE4kKhkADMjGvPmYG788vn15Jixg0XHKFgEJFDTqGQIMyM686dSUen8+t56zCD689XMIjIoaVQSCBmxrfOO5xOh7teXEdq+FjBICKHikIhwZgZ/3H+4XR6cMTQ3N7B9ecfSUaabk4XkXdOoZCALLymkJmewi+fW8vyrXXcdtnxjCnMjro0EUlw+nqZoMyMb5x7OLdddjyrdjRw3i3zmLdqV9RliUiCUygkuA8cPYa/XH06pXkZfOKuV/nZ06vo1EB6InKQFArDwNSyPB6+6nTmHlfOzU++xWfueZ2axtaoyxKRBKRQGCZyMtL40SXH8v25RzFv9S7Ou2UeSzbXRF2WiCQYhcIwYmZ84pSJ/PHK0wC4+Bcvc9+rG3DX6SQRGRiFwjB03PgiHv3CGZwytYRv/XkZ1/xxMU2tHVGXJSIJQKEwTI3IzeA3l5/Il8+axp8XbmHuz19k8SadThKR/ikUhrHUFOPLZ03n7k+fxO7GVube9iLf+vNSahvboi5NRIYohUISePf0Mp6+5t1cftok7n9tI2fe/Cx/rNikaw0i8jYKhSRRkJXODRccyaNfOIOJJTl8/cElXPLLl3lze13UpYnIEKJQSDJHji3kwStP478/dDSrdwZ3Qt/41zdoaGmPujQRGQIUCkkoJcX4yIkT+Oc1c7hk9jh+9cI6zrr5Of66ZJtOKYkkOYVCEivOzeC//s8xPPT50xiRm8FVv1vAJ+96jdU7G6IuTUQiolAQTphYzCNXn84NFxzBoo01vO/Hz/HF+xeyakd91KWJyCCLWyiY2V1mttPMlvWx3szsFjNbbWZLzOz4eNUi+5eWmsKnT5/MM1+fw+feNZWnVuzg7J88z1X3LdDFaJEkEs8jhbuBc/pZfy4wLZyuAH4Rx1pkgErzMrnu3JnMu/ZM/m3OVJ57q5JzfvICV/7vfJZvrY26PBGJs7iFgrs/D+zup8mFwL0eeAUoMrMx8apHDsyI3Ay+/v6ZzLv2PXzxvdN4cU0wyN5n76lg6WaFg8hwFeU1hXJgU8zjzeGytzGzK8yswswqKisrB6U4CRTlZPDV901n3rVn8pWzpvPauiouuHUe/3r36yzcWB11eSJyiCXEhWZ3v8PdZ7v77LKysqjLSUqF2el86axpvHjdmXz9/TNYsLGai257iQtvnccDFZs04J7IMBFlKGwBxsc8HhcukyEsPyudq95zGPOuPZMbLjiCPa0d/PuDSzj5P5/iu48uZ/VO9VgSSWRpEb72I8DVZvZ74GSg1t23RViPHIC8zDQ+ffpkLj9tEq+t281vX93Ib1/ZwG9eXM8pU0Zw2ckTef+Ro8lIS4iDUREJxS0UzOx+YA5QamabgRuAdAB3vx14HPgAsBpoBD4dr1okfsyMk6eUcPKUEnY1HMEDFZv43asb+cL9CynNy+CS2eP56EkTGD8iJ+pSRWQALNGGNZg9e7ZXVFREXYb0o7PTeW5VJfe9spF/vrkDB06fWsoFx47hnCPHUJiTHnWJIknHzOa7++z9tlMoSDxtrWni969v4i+LtrChqpH0VOPd08u44NixnHX4KHIzozyDKZI8FAoypLg7S7fU8ujirTy2ZBvbapvJSk/hvTNHccGxY5gzYyRZ6alRlykybCkUZMjq7HTmb6zm0cVbeXzpNnY1tJKXmcbZR4zi/GPHcNrUUgWEyCGmUJCE0N7RyStrd/Po4q08sWwbdc3tZKWncMZhpbxn5kjOnDmSMYXZUZcpkvAUCpJwWts7eWnNLp55cydPv7mTzdVNABw+poAzZ5Zx5sxRHDe+iNQUi7hSkcSjUJCE5u6s3tnAP8OAmL+hmo5OZ0RuBu+eXsaZM0fyL9NKKcrJiLpUkYSgUJBhpbaxjedXVfLPN3fy7MqdVDe2YQaHjy7g1KklnDa1hBMnj6AgS91dRXqjUJBhq6PTWbSphpdW7+KlNVXM31hNa3snKQZHlxdy6tRSTp1awomTisnJUJdXEVAoSBJpbutg4cYaXl6zi5fXVrFwYw3tnU5ainHc+CJOmVLCCZOKOX58sW6ck6Q10FDQ1yhJeFnpqZw6tYRTp5YA0NjaTsX6al5eW8VLa6q47dnVdIbffaaW5XLCxGKOn1DM8ROLOawsjxRduBbppiMFGfb2tLSzeHMNCzfWMH9DNQs2VlPT2AZAQVYax00o5vgJRZwwsZhjxhVRmK2jCRl+dKQgEsrNTOO0qaWcNrUUCHo2rdu1hwVhSCzcWM1Pn15F1/ejiSU5HDW2kKPKCzm6vJCjygvUy0mShkJBko6ZMaUsjylleVx8wjgA6pvbWLypliVbali2Jfj716V7R3IfV5wdBkQ4jS2gJC8zql0QiRuFggjBjwedMa2UM6aVdi+raWxl2ZY6lm6pZdnWWpZtqeWJZdu715flZzJzdD4zR+czY3QBM0fnc9jIPA3RIQlNoSDSh6KcjLcFRW1TG8u31LJ8ax1vbq9n5Y467n15Ay3tnQCkphiTSnKYObqAGWFgTB+Vz/gROboTWxKCQkHkABRmp3PaYaWcdtjeoGjv6GR9VSMrt9ezcnsQFku31O5z+ikjNYXJpbkcNjKPqSPzmFoWzpfpyEKGFoWCyDuUlprCYSPzOGxkHucdM6Z7+Z6Wdt7aUc+qnQ2sqWxgzc4Glm+t5Yll27q7yJpBeVF2d0BMKs1lckkuk0pzGFuYre6yMugUCiJxkpuZxqwJxcyaULzP8ua2DjZUNbJ6ZwOrw8BYvbOBV9ZW0dzW2d0uIy2FiSNymFSay6SSnJjAyGV0QZYCQ+JCoSAyyLLSU5kxOp8Zo/P3Wd7Z6eyob2bdrj2s39XI+qo94fwennurktb2fQNjXHE244tzmDAih/EjssO/waQxoORgKRREhoiUFGNMYTZjCrM5beq+6zo7na21TWyoamTdrj1s2t3Ixt2NbKpuZOHGauqa2/dpX5STzvjiICzKi8KpOIexRVmMK8qhIDsNMx1pyNspFEQSQEqKMa44h3HFOZwec5G7S21jG5uqw6DoDowm3txWz9Mrdnb3juqSl5nG2KKsMCyyGRsGx+iCLMYWZTOyIJPMNF0AT0YKBZFhoDAnncKc4Ma6ntydqj2tbKluYktN096/4fyCjTXUNrW97XmleRmMKcxmdGEWYwqzGF2Yxdjw8aiCLEYVZGoU2mFI76jIMGdmlOZlUpqXybHji3pt09DSzvbaJrbVNrOtpplttc1sr2tia00zG6saeXVt1dtOUQHkZ6YxsiAzDImsYD5/b2iMzM+iLD+T7AwddSQKhYKIkJeZxmEj8zlsZH6fbfa0tAdhUdvMzvpmdtS1sKNu7/zr63ezs66F1o7Otz03PzONsvxMSvMzKcvPpCwvk5EFwd+ycFlpXiYjcjNIT02J567KfigURGRAcjPTuu/H6Iu7U9PYxo76IDwq61uobGgJ/ta3sLO+hRVb63i+voX6lrcfeUBwkbw0L5OS3AxK8zMpzc0IHudlUpqXQUleBiNygwApyNIF80NNoSAih4yZUZybQXFuBjNHF/Tbtqm1g10NLeysD8JjV0MruxpaqIr5u2JrHZUNLdT3cuoKID3VKM7JYETu3rAoyQ0ed03FORkU56YzIieDopwMMtJ0JNIfhYKIRCI7I7X7vor9aWnv2BsWe1rZ3dDK7j2twfyelu75LdU1VO1p7TNEIDhV1hUSxWFoFOWkd/8tzE7fZ1lhTjr5mclzRKJQEJEhLzMtlbFFQdfZgWht76S6sZWW4m4xAAAJ9klEQVTqxiA8qve0sbuxlZo9rexubKV6TyvVjW3s3tPK6p0N1DS20dDH6SwIBjosyk4Penllpwfz2ekU5WRQEPM4WLZ3viA7PeHGtlIoiMiwk5GW0t0jaqDaOjqpbWqjprGVmsY2qhuD+dqmNqrDZTWNbdQ2tbGroZXVlQ3UNrZR39JOfz9gmZGWQkFWOgXZaUFQZAVhUZid1j2fn9XLfFYaBdnpZKalDOpRikJBRARIT03p7rp7IDo6nfrmtjBQwr9NbdQ1tVEXLq9rau9+XNPYyoaqPdQ1t1Pb1EZHZ/8/iZyeat2BcdnJE/jsv0x5J7u5XwoFEZF3IDXFKAovYk8sObDnujuNrR3UN7dT19xGfXMYIM1t1DW3dz+uDx+X5cf/1/4UCiIiETEzcjPTyM1MY3ThwE91xZP6ZomISDeFgoiIdItrKJjZOWa20sxWm9l1vay/3MwqzWxROH02nvWIiEj/4nZNwcxSgZ8D7wM2A6+b2SPu/kaPpn9w96vjVYeIiAxcPI8UTgJWu/tad28Ffg9cGMfXExGRdyieoVAObIp5vDlc1tOHzGyJmT1oZuN725CZXWFmFWZWUVlZGY9aRUSE6C80PwpMcvdjgCeBe3pr5O53uPtsd59dVlY2qAWKiCSTeIbCFiD2m/+4cFk3d69y95bw4a+BE+JYj4iI7Ec8b157HZhmZpMJwuBS4GOxDcxsjLtvCx9+EFixv43Onz9/l5ltOMiaSoFdB/ncoWq47dNw2x8Yfvs03PYHht8+9bY/EwfyxLiFgru3m9nVwN+BVOAud19uZt8DKtz9EeCLZvZBoB3YDVw+gO0e9PkjM6tw99kH+/yhaLjt03DbHxh++zTc9geG3z69k/2J6zAX7v448HiPZdfHzH8D+EY8axARkYGL+kKziIgMIckWCndEXUAcDLd9Gm77A8Nvn4bb/sDw26eD3h/z/n4dQkREkkqyHSmIiEg/FAoiItItaUJhfyO2JiIzW29mS8MRZiuirudAmdldZrbTzJbFLBthZk+a2arwb3GUNR6oPvbpO2a2JWY04A9EWeOBMLPxZvaMmb1hZsvN7Evh8oR8n/rZn0R+j7LM7DUzWxzu03fD5ZPN7NXwM+8PZpYxoO0lwzWFcMTWt4gZsRX4aC8jtiYUM1sPzHb3hLzpxszeBTQA97r7UeGyHwC73f2mMLyL3f3aKOs8EH3s03eABnf/nyhrOxhmNgYY4+4LzCwfmA/MJbinKOHep3725xIS9z0yINfdG8wsHZgHfAn4KvAnd/+9md0OLHb3X+xve8lypKARW4cgd3+e4KbFWBeydwysewj+h00YfexTwnL3be6+IJyvJxh1oJwEfZ/62Z+E5YGG8GF6ODlwJvBguHzA71GyhMJAR2xNNA78w8zmm9kVURdziIyKGfpkOzAqymIOoavD0YDvSpRTLT2Z2SRgFvAqw+B96rE/kMDvkZmlmtkiYCfB4KJrgBp3bw+bDPgzL1lCYbg6w92PB84FrgpPXQwbHpzbHA7nN38BTAWOA7YBN0dbzoEzszzgIeDL7l4Xuy4R36de9ieh3yN373D34wgGHj0JmHmw20qWUNjviK2JyN23hH93An8m+I8h0e0Iz/t2nf/dGXE975i77wj/p+0EfkWCvU/heeqHgPvc/U/h4oR9n3rbn0R/j7q4ew3wDHAqUGRmXUMZDfgzL1lCoXvE1vAK/KXAIxHX9I6YWW54oQwzywXOBpb1/6yE8AjwqXD+U8BfIqzlkOj68AxdRAK9T+FFzDuBFe7+o5hVCfk+9bU/Cf4elZlZUTifTdChZgVBOFwcNhvwe5QUvY8Awi5mP2HviK03RlzSO2JmUwiODiAY2PB3ibZPZnY/MIdgmN8dwA3Aw8ADwARgA3CJuyfMhds+9mkOwWkJB9YDn4s5Hz+kmdkZwAvAUqAzXPxNgvPwCfc+9bM/HyVx36NjCC4kpxJ80X/A3b8Xfkb8HhgBLAQ+HvP7NX1vL1lCQURE9i9ZTh+JiMgAKBRERKSbQkFERLopFEREpJtCQUREuikUZMgws5fCv5PM7GOHeNvf7O214sXM5prZ9ftveVDb/ub+Wx3wNo82s7sP9XYl8ahLqgw5ZjYH+Jq7n38Az0mLGeelt/UN7p53KOobYD0vAR98pyPY9rZf8doXM3sK+Fd333ioty2JQ0cKMmSYWddIjzcB/xKOa/+VcLCvH5rZ6+GAZZ8L288xsxfM7BHgjXDZw+EAgcu7Bgk0s5uA7HB798W+lgV+aGbLLPhtio/EbPtZM3vQzN40s/vCu2Exs5ssGI9/iZm9bahlM5sOtHQFgpndbWa3m1mFmb1lZueHywe8XzHb7m1fPm7BePqLzOyXFgwVj5k1mNmNFoyz/4qZjQqXfzjc38Vm9nzM5h8luNtfkpm7a9I0JCaC8ewhuAP4sZjlVwDfDuczgQpgcthuDzA5pu2I8G82wVAFJbHb7uW1PkQwqmQqwUifG4Ex4bZrCcaMSQFeBs4ASoCV7D3KLuplPz4N3Bzz+G7gb+F2phGMWJl1IPvVW+3h/OEEH+bp4ePbgE+G8w5cEM7/IOa1lgLlPesHTgcejfq/A03RTl2DJYkMZWcDx5hZ1zguhQQfrq3Aa+6+LqbtF83sonB+fNiuqp9tnwHc7+4dBIO8PQecCNSF294MEA5LPAl4BWgG7jSzx4DHetnmGKCyx7IHPBhsbZWZrSUYxfJA9qsv7wVOAF4PD2Sy2Ts4XWtMffMJxsQBeBG428weAP60d1PsBMYO4DVlGFMoSCIw4Avu/vd9FgbXHvb0eHwWcKq7N5rZswTfyA9W7DgxHUCau7eb2UkEH8YXA1cT/JhJrCaCD/hYPS/eOQPcr/0w4B53/0Yv69rcvet1Owj/f3f3K83sZOA8YL6ZneDuVQT/Vk0DfF0ZpnRNQYaieiA/5vHfgc+HQx5jZtPDkWF7KgSqw0CYCZwSs66t6/k9vAB8JDy/Xwa8C3itr8IsGIe/0N0fB74CHNtLsxXAYT2WfdjMUsxsKjCF4BTUQPerp9h9eRq42MxGhtsYYWYT+3uymU1191fd/XqCI5quYeWnk0Cjg0p86EhBhqIlQIeZLSY4H/9TglM3C8KLvZX0/tOCfwOuNLMVBB+6r8SsuwNYYmYL3P2ymOV/Jhh7fjHBt/d/d/ftYaj0Jh/4i5llEXxL/2ovbZ4HbjYzi/mmvpEgbAqAK9292cx+PcD96mmffTGzbxP8Al8K0AZcRTByaV9+aGbTwvqfDvcd4D3AXwfw+jKMqUuqSByY2U8JLto+Ffb/f8zdH9zP0yJjZpnAcwS/5tdn114Z/nT6SCQ+/hPIibqIAzABuE6BIDpSEBGRbjpSEBGRbgoFERHpplAQEZFuCgUREemmUBARkW7/H23jCY7YAMzFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111aa0d68>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 4000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuray we get on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8588000000000001\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8388000000000002\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_set_x, test_set_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ~87%. You can train the model even longer and get better result. You can also try to change the network structure. \n",
    "<br>Below, you can see which all numbers are incorrectly identified by the neural network by changing the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1051e14e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105264780>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 3474\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
