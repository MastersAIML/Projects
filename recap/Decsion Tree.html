<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.4 (456999)"/><meta name="altitude" content="891.0181884765625"/><meta name="author" content="montygupta@gmail.com"/><meta name="created" content="2018-07-21 09:24:47 +0000"/><meta name="latitude" content="12.91206873762878"/><meta name="longitude" content="77.6390816868103"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-07-27 01:00:35 +0000"/><title>Decsion Tree</title></head><body><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"/><p style="box-sizing: border-box; color: rgb(135, 135, 135); font-family: Lato, sans-serif; font-size: 14px; font-style: italic; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 254, 226); text-decoration-style: initial; text-decoration-color: initial;">The answer contains the parameter used in Python implementation and it's explanation:
</p><div><ol style="box-sizing: border-box; color: rgb(135, 135, 135); font-family: Lato, sans-serif; font-size: 14px; font-style: italic; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 254, 226); text-decoration-style: initial; text-decoration-color: initial;"><li style="box-sizing: border-box;"><strong style="box-sizing: border-box; font-weight: 700;">max_depth</strong>: The maximum depth of the tree</li><li style="box-sizing: border-box;"><strong style="box-sizing: border-box; font-weight: 700;">min_samples_split</strong>: The minimum number of samples required to split an internal node</li><li style="box-sizing: border-box;"><strong style="box-sizing: border-box; font-weight: 700;">min_samples_leaf</strong>: The minimum number of samples required to be at a leaf node:</li><li style="box-sizing: border-box;"><strong style="box-sizing: border-box; font-weight: 700;">min_weight_fraction_leaf</strong>: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node</li><li style="box-sizing: border-box;"><strong style="box-sizing: border-box; font-weight: 700;">max_leaf_nodes</strong>: The maximum number of leaf nodes that can be generated</li><li style="box-sizing: border-box;"><strong style="box-sizing: border-box; font-weight: 700;">min_impurity_decrease</strong>: A node will be split if this split induces a decrease of the impurity greater than or equal to this value</li><li style="box-sizing: border-box;"><strong style="box-sizing: border-box; font-weight: 700;">min_impurity_split</strong>: A node will split if its impurity is above the threshold, otherwise it is a leaf</li></ol></div><p style="box-sizing: border-box; color: rgb(135, 135, 135); font-family: Lato, sans-serif; font-size: 14px; font-style: italic; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 254, 226); text-decoration-style: initial; text-decoration-color: initial;">Please refer to the link mentioned just before the quiz to get a better understanding.
</p><br/><p/><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">In regression problems, a decision tree splits the data into multiple subsets. The difference between decision tree classification and decision tree regression is that in regression,<strong style="box-sizing: border-box; font-weight: 700;"> each leaf represents a linear regression model</strong>, as opposed to a class label.
</p><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p></div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">In this module, you won't study decision tree regression in detail, but only decision tree classification because that is what you’ll most commonly work on. However, remember that if you get a data set where you want to perform linear regression on multiple subsets, decision tree regression is a good idea.
</p></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">This was a brief introductory session on decision trees. Let’s revisit some of the concepts that we covered in this session:
</p><div/><ol style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"><li style="box-sizing: border-box;">Decision trees are easy to interpret; you can always go back and identify the various factors leading to a decision.</li><li style="box-sizing: border-box;">A decision tree requires you to perform tests on attributes in order to split the data into multiple partitions.</li><li style="box-sizing: border-box;">In classification, each data point in a leaf has a class label associated with it.</li><li style="box-sizing: border-box;">There are some cases where a linear regression model cannot be used to make predictions, such as when you want to divide the data set into multiple subsets because each subset has an independent trend corresponding to it. There, you use a decision tree model to make predictions because a tree regression model has the capability of splitting the data into multiple sets and assigning a linear regression model to each set independently.</li></ol></div><div><br/></div><div>High Variance in data is low homogeneity</div><div>High Homegenity means labels are same</div><div><br/></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;">More homogeneity will mean that most of the data points in the set belong to the same class label. Hence, classifying all the data points of that set, to get them to belong to that class, will result in lesser errors.</em><br/></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><br/></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"/></div><div><a href="http://www.slideshare.net/21_venkat/decision-tree-53154033">http://www.slideshare.net/21_venkat/decision-tree-53154033</a><br/></div><div><br/></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">In this session, you saw how a decision tree is constructed. You go step-by-step, picking an attribute and a rule to split the data into multiple partitions, increasing the homogeneity of the data set.
</p><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">You also learnt about the various ways you can measure the homogeneity of the data set, such as the Gini index, information gain, and <span style="box-sizing: border-box;"><span style="box-sizing: border-box; text-indent: 0px; text-align: left; text-transform: none; font-style: normal; font-weight: 400; font-size: 20px; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 1px 0px;"><span style="box-sizing: border-box; border-collapse: separate; border-spacing: 0px;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left; white-space: pre; font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span><span style="box-sizing: content-box !important; text-align: left; font-size: 14.14px; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left; white-space: pre; font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span>. 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">So, the following steps are involved in decision tree construction:
</p><ol style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"><li style="box-sizing: border-box;">A decision tree first decides on an attribute to split on.</li><li style="box-sizing: border-box;">To select this attribute, it measures the homogeneity of the nodes before and after the split.</li><li style="box-sizing: border-box;">There are various ways in which you can measure the homogeneity.</li><li style="box-sizing: border-box;">You have the Gini index and information gain for classification; you also have <span style="box-sizing: border-box;"><span style="box-sizing: border-box; text-indent: 0px; text-align: left; text-transform: none; font-style: normal; font-weight: 400; font-size: 20px; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 1px 0px;"><span style="box-sizing: border-box; border-collapse: separate; border-spacing: 0px;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left; white-space: pre; font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span><span style="box-sizing: content-box !important; text-align: left; font-size: 14.14px; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left;"><span style="box-sizing: content-box !important; text-align: left; white-space: pre; font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span> for the regression, to measure the homogeneity.</li><li style="box-sizing: border-box;">The attribute that results in a maximum homogeneous data set is then selected for splitting.</li><li style="box-sizing: border-box;">Then, this whole cycle is repeated till you get a sufficiently homogeneous data set.</li></ol><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p></div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">Which homogeneity measure should you choose for splitting (for classification), the Gini index or information gain
</p></div><div><br/></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">To summarise the advantages,
</p><div/><ul style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"><li style="box-sizing: border-box;" dir="ltr"><p style="box-sizing: border-box; font-size: 1em; font-weight: 300; word-wrap: break-word;" dir="ltr">Predictions made by a decision tree are easily interpretable.
</p></li><li style="box-sizing: border-box;" dir="ltr"><p style="box-sizing: border-box; font-size: 1em; font-weight: 300; word-wrap: break-word;" dir="ltr">A decision tree does not assume anything specific about the nature of the attributes in a data set. It can seamlessly handle all kinds of data — numeric, categorical, strings, Boolean, and so on.
</p></li><li style="box-sizing: border-box;" dir="ltr"><p style="box-sizing: border-box; font-size: 1em; font-weight: 300; word-wrap: break-word;" dir="ltr">It does not require normalisation since it has to only compare the values within an attribute.
</p></li><li style="box-sizing: border-box;" dir="ltr"><p style="box-sizing: border-box; font-size: 1em; font-weight: 300; word-wrap: break-word;" dir="ltr">Decision trees often give us an idea of the relative importance of the explanatory attributes that are used for prediction.
</p></li></ul></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">To summarise the disadvantages,
</p><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"> 
</p></div><ul style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"><li style="box-sizing: border-box;" dir="ltr"><p style="box-sizing: border-box; font-size: 1em; font-weight: 300; word-wrap: break-word;" dir="ltr">Decision trees tend to overfit the data. If allowed to grow with no check on its complexity, a tree will keep splitting till it has correctly classified (or rather, mugged up) all the data points in the training set.
</p></li><li style="box-sizing: border-box;" dir="ltr"><p style="box-sizing: border-box; font-size: 1em; font-weight: 300; word-wrap: break-word;" dir="ltr">Decision trees tend to be very unstable, which is an implication of overfitting. A few changes in the data can change a tree considerably.
</p></li></ul></div><div><span style="box-sizing: border-box; font-weight: 700; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"> </span><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;">The most informative features are towards the top of a tree.</em><br/></div><div><br/></div><div><span style="color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial; float: none;">The model has memorised the data, giving you a 98% training accuracy and leading to a high variance. Since it can now represent the training set very well, it has a low bias.</span><br/></div><div><span style="color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial; float: none;"><br/></span></div><div><span style="color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial; float: none;"><span style="color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial; float: none;">The test accuracy is very low (40%). The model is unable to work well on unseen/test data. It has memorised the training set. Hence, it is overfitting.</span><br/></span></div><div><br/></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">There are two ways to control overfitting in trees:
</p><div/><ol style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"><li style="box-sizing: border-box;" dir="ltr"><p style="box-sizing: border-box; font-size: 1em; font-weight: 300; word-wrap: break-word;" dir="ltr"><strong style="box-sizing: border-box; font-weight: 700;">Truncation</strong> - Stop the tree while it is still growing so that it may not end up with leaves containing very few data points.
</p></li><li style="box-sizing: border-box;" dir="ltr"><p style="box-sizing: border-box; font-size: 1em; font-weight: 300; word-wrap: break-word;" dir="ltr"><strong style="box-sizing: border-box; font-weight: 700;">Pruning</strong> - Let the tree grow to any complexity. Then, cut the branches of the tree in a bottom-up fashion, starting from the leaves. It is more common to use pruning strategies to avoid overfitting in practical implementations.</p></li></ol></div><div>Methods of Truncation:</div><div><ol start="1"><li>Limit the minimum size of a partition after a split</li><li>Minimize change in the measure of homogeneity</li><li>Limit the depth of a tree</li><li>Minimum threshold of members in the leaf</li><li>Maximum number of leaves in a tree</li></ol></div><div><br/></div><div><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;">You check the performance of a pruned tree on a validation set. If the accuracy of the pruned tree is higher than the accuracy of the original tree (on the validation set), then you keep that branch chopped. Remember that the validation set is the third part of the data set, the first and second being the training and test set.</span><br/></div><div><br/></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">We will first have to clean and prepare the data in a format which sklearn can understand.
</p><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">Let's now understand the two most common ways of preparing categorical variables - <strong style="box-sizing: border-box; font-weight: 700;">dummy variables</strong>/<strong style="box-sizing: border-box; font-weight: 700;">one hot encoding </strong>and <strong style="box-sizing: border-box; font-weight: 700;">label encoding</strong>. 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">You have already created dummy variables (for replacing categorical variables) in models such as linear and logistic regression. Note that creating dummy variables is also called <strong style="box-sizing: border-box; font-weight: 700;">one-hot encoding</strong>.
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p></div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">However, you should avoid doing that in decision trees (or other tree-based, non-linear models such as random forests), since there's a better way to represent categorical variables in tree models -<strong style="box-sizing: border-box; font-weight: 700;"> label encoding. </strong>
</p></div><div/><div><ul style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"><li style="">Decision trees are prone to overfitting.</li><li style="">There are two ways to avoid overfitting: truncation and pruning.</li><li style="">In truncation, you let the tree grow only to a certain size, while in pruning, you let the tree grow to its logical end and then chop off the branches that do not increase accuracy on the validation set.</li><li style="">There are various hyperparameters in the DecisionTreeClassifier that let you truncate the tree, namely minsplit, max_depth, etc.</li></ul></div><div><br/></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">Now, to understand how an ensemble makes decisions, consider an ensemble with 100 models comprising of decision trees, logistic regression models, etc. Given a new data point, each model will predict an output y for this data point. If this is binary classification, then you simply take the majority score. If more than 50% models say y = 0, you go with 0 and vice-versa.
</p><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">The question is, why should you expect the majority vote to perform better on unseen data than any of the individual 100 models? There are a number of convincing arguments to answer this.
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">Firstly, if each of the individual models is <strong style="box-sizing: border-box; font-weight: 700;">acceptable</strong>, i.e.they’re wrong with a probability less than 50%, you can show that the probability of the ensemble being wrong (i.e. the majority vote going wrong) will be far lesser than that of any individual model.
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"> 
</p></div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">Also, the ensembles avoid getting misled by the <strong style="box-sizing: border-box; font-weight: 700;">assumptions made by individual models</strong>. For example, ensembles (particularly random forests) successfully reduce the problem of overfitting. If a decision tree in an ensemble overfits, you let it. Chances are extremely low that more than 50% of the models have overfitted. Ensembles make sure that you do not put all your eggs in one basket.
</p></div><div><h2 style="box-sizing: border-box; font-family: Lato, sans-serif; font-weight: 300 !important; color: rgb(155, 155, 155); font-size: 26px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">Bagging
</h2><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"/><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">To understand random forests, you will first need to understand <strong style="box-sizing: border-box; font-weight: 700;">bagging</strong>, an ensemble method. Bagging stands for <strong style="box-sizing: border-box; font-weight: 700;">b</strong><strong style="box-sizing: border-box; font-weight: 700;"><strong style="box-sizing: border-box; font-weight: 700;">ootstrapped</strong> aggregation. </strong>It is a technique for choosing random samples of observations from a dataset. Each of these samples is then used to train each tree in the forest. 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"> 
</p></div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">As you study bagging, keep in mind that it is just a <strong style="box-sizing: border-box; font-weight: 700;">sampling technique</strong> and is <strong style="box-sizing: border-box; font-weight: 700;">not specific to random forests.</strong>
</p></div><div><img src="Decsion%20Tree.resources/Screen%20Shot%202018-07-22%20at%2010.40.34%20AM.png" height="772" width="646"/><br/></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">Bagging includes the creation of different bootstrap samples for different models, and aggregating the results of the models. Random forests use this technique along with randomly selecting features at each node while splitting it.</em><br/></div><div><br/></div><div><img src="Decsion%20Tree.resources/Screen%20Shot%202018-07-22%20at%2010.54.49%20AM.png" height="632" width="678"/><br/></div><div><br/></div><div><br/></div><div><img src="Decsion%20Tree.resources/Screen%20Shot%202018-07-22%20at%2010.59.53%20AM.png" height="286" width="612"/><br/></div><div><span style="color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial; float: none;">The word ‘random’ in random forests refers to 1) the random choice of bootstrapped observations and 2) random choice of attributes at each split of a tree.</span><br/></div><div><br/></div><div><span style="box-sizing: border-box; font-weight: 700; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"> </span><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;">If you have only one tree, you have to rely on the decision it makes. The decision a single tree makes (on unseen data) depend highly on the training data since trees are unstable. In a forest, even if a few trees are unstable, averaging out their decisions ensures that you are not making mistakes because of a few trees’ unstable behaviou</em><br/></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><br/></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">While it is well known that random forests are better, in terms of accuracy, than a single decision tree, it cannot be said that they are better than every possible decision tree. It is just more difficult to build a decision tree that is better than a random forest. In fact, there may be several trees that provide better predictions on unseen data</em><br/></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></em></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;">the OOB error is calculated as the number of observations predicted wrongly as a proportion of the total number of observations.</span><br/></em></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;"><br/></span></em></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;"><div style="box-sizing: border-box; padding-top: 4px; border-top: 1px solid rgb(238, 238, 238); font-weight: normal;"><span style="box-sizing: border-box; vertical-align: top; font-weight: normal; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial;"><em style="box-sizing: border-box; font-size: 14px;">Recall that all the observations of the training set are used to calculate the OOB error.</em></span></div><div/><div><br/></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;">The time required will obviously depend on S. While building each of the S trees, time is spent in creating the levels of trees and time required to find splits among f features. Levels of trees are given by log(n). Finding the right split depends on both n observations and f features because homogeneity will be measured for all f features and n observations.</em><br/></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><br/></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;"><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(244, 244, 244); text-decoration-style: initial; text-decoration-color: initial;">Each split is made by comparing the homogeneity across j= 40% of the n observations. Thus, it has to depend on j and n (more the observations, more the time required to compare homogeneity). The time required to find a split also depends upon the number of features being considered which is sqrt(M).</em><br/></em></div></span></em></em></div><div><br/></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"><strong style="box-sizing: border-box; font-weight: 700;">The Effect of max_features</strong></p><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">You saw that there is an optimal value of max_features - at very <strong style="box-sizing: border-box; font-weight: 700;">low values</strong>, the component trees are<strong style="box-sizing: border-box; font-weight: 700;"> too simple to learn anything useful</strong>, while at extremely high values, the component trees <strong style="box-sizing: border-box; font-weight: 700;">become similar to each other </strong>(and violate the 'diversity' criterion). 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"><strong style="box-sizing: border-box; font-weight: 700;">The Effect of n_estimators</strong></p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p></div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">Also, when you observe the plot of n_estimators and training and test accuracies, you will see that the as you increase the value of n_estimators, both the training test accuracies gradually increase. More importantly, the model does <em style="box-sizing: border-box;">not </em>overfit even when its complexity is increasing. This is an important benefit of random forests - you can increase the number of trees as much you like without worrying about overfitting (if your computational resources allow). 
</p></div><div><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;">There is one more ensemble method: '</span><strong style="box-sizing: border-box; font-weight: 700; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">Boosting'</strong><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;">. Boosting is also a very popular method in online machine learning competitions, such as Kaggle. The basic idea is to </span><strong style="box-sizing: border-box; font-weight: 700; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">combine a lot of weak learners to get a strong learner</strong><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;">. </span><strong style="box-sizing: border-box; font-weight: 700; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">Weak learner</strong><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;"> refers to a model that is slightly better than a model that predicts at random. Weak learners are built sequentially on top of each other, giving a </span><strong style="box-sizing: border-box; font-weight: 700; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">boost</strong><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;"> to the overall performance of the model.</span><br/></div><div><br/></div></body></html>