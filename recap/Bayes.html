<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.4 (456999)"/><meta name="altitude" content="880.6220092773438"/><meta name="author" content="montygupta@gmail.com"/><meta name="created" content="2018-06-30 04:40:58 +0000"/><meta name="latitude" content="12.91064951327892"/><meta name="longitude" content="77.68213309303867"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-07-01 03:28:37 +0000"/><title>Bayes</title></head><body><div>Conditional Probability</div><div><br/></div><div><a href="https://people.richland.edu/james/lecture/m170/ch05-rul.html">https://people.richland.edu/james/lecture/m170/ch05-rul.html</a><br/></div><div><br/></div><div><a style="box-sizing: border-box; background-color: rgb(245, 245, 245); color: rgb(35, 82, 124); text-decoration: underline; outline: -webkit-focus-ring-color auto 5px; outline-offset: -2px; font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;" href="http://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification">StackOverflow</a><br/></div><div><br/></div><div><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;">Naïve Bayes follows an assumption that the variables are conditionally independent given the class i.e. P(X= convex,smooth/C= edible ) can be written as P(X=smooth/C=edible)</span><img src="Bayes.resources/38831D3E-9DA8-4E0B-8D27-B030E1CC8614.gif" height="9" width="10"/><span style="color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial; float: none;">P(X=convex/C=edible). The terms P(X=smooth/C=edible) and P(X=convex/C=edible) is simply calculated by counting the data points. Hence, the name “Naïve” because in most real-world situations the variables are not conditionally independent given the class label but most of the times the algorithm works nonetheless. Despite this assumption, Naive Bayes has proven to work very well in some cases, such as text classification. You'll study an example of classifying emails into spam/ham in the next session</span><br/></div><div><br/></div><div><p style="box-sizing: border-box; font-size: 14px; font-weight: 500; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">In the previous questions, you have calculated that:
</p><div><p style="box-sizing: border-box; font-size: 14px; font-weight: 500; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">What is P(CONVEX | edible) = 4/8
</p><p style="box-sizing: border-box; font-size: 14px; font-weight: 500; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">P(SMOOTH| edible) = 2/8
</p><p style="box-sizing: border-box; font-size: 14px; font-weight: 500; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">P(CONVEX | poisonous) = 1 and
</p><p style="box-sizing: border-box; font-size: 14px; font-weight: 500; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">P(SMOOTH| poisonous) = 1/4
</p></div><p style="box-sizing: border-box; font-size: 14px; font-weight: 500; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;"><br style="box-sizing: border-box;"/><br/>If all mushrooms above 50% probability of being edible are classified as edible, is the CONVEX, SMOOTH mushroom edible?
</p><br/></div><div><br/></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">P(edible).P(CONVEX | edible).P(SMOOTH| edible) = (8/12)(4/8)(2/8) = 1/12 P(poisonous).P(CONVEX | poisonous). P(SMOOTH| poisonous) = (4/12)(1)(1/4) = 1/12 Since both numerators are equal to 1/12, this mushroom cannot be classified with a 50% threshold. Although if you would take a higher threshold, like 60% (which is reasonable since you don't want to take responsibility of people eating poisonous mushrooms), then it will be classified as poisonous.</em><br/></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="color: rgb(51, 51, 51); font-family: Lato;"><i><a href="https://learn.upgrad.com/v/course/132/session/15393/segment/77593">https://learn.upgrad.com/v/course/132/session/15393/segment/77593</a></i></font><br/></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="color: rgb(51, 51, 51); font-family: Lato;"><br/></font></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="color: rgb(51, 51, 51); font-family: Lato;"><br/></font></em></div><div><em style="box-sizing: border-box; font-size: 14px; color: rgb(51, 51, 51); font-family: Lato, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="color: rgb(51, 51, 51); font-family: Lato;"><h2 style="box-sizing: border-box; font-family: Lato, sans-serif; font-weight: 300 !important; color: rgb(155, 155, 155); font-size: 26px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">Prior, Posterior and Likelihood
</h2><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">Let’s understand the terminology of Bayes theorem.
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">You have been using 3 terms: P(Class = edible / poisonous), P(X | Class) and P(Class | X). Bayesian classification is based on the principle that ‘you combine your <strong style="box-sizing: border-box; font-weight: 700;">prior knowledge or beliefs about a population</strong> with the <strong style="box-sizing: border-box; font-weight: 700;">case specific information</strong> to get the actual (posterior) probability’.
</p><ul style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"><li style="box-sizing: border-box;">P(Class = edible/poisonous) is called the <strong style="box-sizing: border-box; font-weight: 700;">prior probability</strong></li></ul><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">This incorporates our ‘<strong style="box-sizing: border-box; font-weight: 700;">prior beliefs</strong>’ before you collect specific information. If 90% of mushrooms are edible, then the prior probability is 0.90. Prior gets multiplied with the likelihood to give the posterior. In many cases, the prior has a tremendous effect on the classification. If the prior is neutral (50% are edible), then the likelihood may largely decide the outcome.
</p><ul style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"><li style="box-sizing: border-box;">P(X|Class) is the <strong style="box-sizing: border-box; font-weight: 700;">likelihood</strong></li></ul><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">After agreeing upon the prior, you collect new, case specific data (like plucking mushrooms randomly from a farm and observing the cap colours). Likelihood updates our prior beliefs with the new information. If you find a CONVEX mushroom, then you’d want to know how likely you were to find a convex one if you had only plucked edible mushrooms.
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">If  P(CONVEX| edible) is high, say 80%, implying that there was an 80% chance of getting a convex mushroom if you only took from edible mushrooms, this will reflect in increased chances of the mushroom being edible.
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">If the likelihood is neutral (e.g. 50%), then the prior probability may largely decide the outcome. If the prior is way too powerful, then likelihood often barely affects the result.
</p><ul style="box-sizing: border-box; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 300; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr"><li style="box-sizing: border-box;">P(Class = edible | X) is the <strong style="box-sizing: border-box; font-weight: 700;">posterior probability</strong></li></ul><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">It is the outcome which<strong style="box-sizing: border-box; font-weight: 700;"> combines prior beliefs and case-specific information</strong>. It is a balanced outcome of the prior and the likelihood.
</p><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p></div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">If Zimbabwe takes 3 Australian wickets in the first over in a world cup, would you predict Australia to lose? Probably not, because the prior odds are way too strong in favour of Australia. They’ve never lost to Zimbabwe in a world cup! The likelihood, though it may be high, gets balanced by the prior odds (Australia’s prior odds may even be 99%!) to give you the correct posterior.
</p><br/></font></em></div><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;">You saw that the most fundamental difference in Bernoulli Naive Bayes Classifier is the way we build the bag of words representation, which in this case is just 0 or 1. Simply put, Bernoulli Naive Bayes is concerned only with whether the word is present or not in a document, whereas Multinomial Naive Bayes counts the no. of occurrences of the words as well.
</p><div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;"> 
</p></div><p style="box-sizing: border-box; font-size: 16px; font-weight: 300; word-wrap: break-word; color: rgb(51, 51, 51); font-family: Merriweather, serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(245, 245, 245); text-decoration-style: initial; text-decoration-color: initial;" dir="ltr">The whole worked out example is out of the scope of this course. But as an optional reading, you can find it here</p></div><div><a href="https://stats.stackexchange.com/questions/105501/understanding-roc-curve">https://stats.stackexchange.com/questions/105501/understanding-roc-curve</a><br/></div><div><br/></div><div><br/></div><div><pre style="box-sizing: border-box; overflow: auto; font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px; padding: 9.5px; color: rgb(51, 51, 51); word-break: break-all; word-wrap: break-word; background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px; font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial;"><span style="box-sizing: border-box; color: rgb(0, 112, 32);">dict</span> <span style="box-sizing: border-box; color: rgb(51, 51, 51);">=</span> CountVectorizer(stop_words<span style="box-sizing: border-box; color: rgb(51, 51, 51);">=</span><span style="box-sizing: border-box; background-color: rgb(255, 240, 240);">'english'</span>)
<span style="box-sizing: border-box; color: rgb(0, 112, 32);">dict</span><span style="box-sizing: border-box; color: rgb(51, 51, 51);">.</span>fit(X_train)
X_train_vocabs_dict <span style="box-sizing: border-box; color: rgb(51, 51, 51);">=</span> <span style="box-sizing: border-box; color: rgb(0, 112, 32);">dict</span><span style="box-sizing: border-box; color: rgb(51, 51, 51);">.</span>get_feature_names()
<span style="box-sizing: border-box; color: rgb(0, 112, 32);">len</span>(X_train_vocabs_dict)
</pre><br/></div><div><br/></div></body></html>